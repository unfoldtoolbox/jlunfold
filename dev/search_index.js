var documenterSearchIndex = {"docs":
[{"location":"lmm_tutorial/#Overlap-Correction-with-Linear-Mixed-Models","page":"LMM Tutorial","title":"Overlap Correction with Linear Mixed Models","text":"","category":"section"},{"location":"lmm_tutorial/#date:-2021-04-29","page":"LMM Tutorial","title":"date: 2021-04-29","text":"","category":"section"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"\n\nusing StatsModels, MixedModels, DataFrames\nimport Plots\nusing Unfold\ninclude(\"../../test/test_utilities.jl\"); # function to load the simulated data","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"This notebook is similar to the lm_tutorial, but fits mass-univariate mixed models and time-expanded/overlap-corrected mixed models.","category":"page"},{"location":"lmm_tutorial/#Reading-input","page":"LMM Tutorial","title":"Reading input","text":"","category":"section"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"The data were simulated in MatLab using the unmixed toolbox (www.unfoldtoolbox.org) with the functionEEG_to_csv.m.","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"Limitation: due to current implementation in MixedModels.jl, we cannot fit overlap-corrected random effects. That is, the (1|item) cannot be modelled at the moment.","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"\ndata, evts = loadtestdata(\"testCase3\",dataPath = \"../../data/test/\")\ndata = data.+ 0.1*randn(size(data)) # we have to add minimal noise, else mixed models crashes.\n\ncategorical!(evts,:subject);","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"The events dataFrame looks like this","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"\nfirst(evts,6)","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"With the important fields being latency, condA, condB and subject.","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"The data are a vector.","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"typeof(data)","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"size(data)","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"Limitation Note how small it is! Only 12k samples, that is only ~5minutes of recording in total for 25 subjects. More realistic samples quickly take hours to fit.","category":"page"},{"location":"lmm_tutorial/#Without-Overlap-Correction","page":"LMM Tutorial","title":"Without Overlap Correction","text":"","category":"section"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"We define the formula","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"f  = @formula 0~1+condA*condB+(1+condA*condB|subject);","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"epoch the data for the mass-univariate mixed model case","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"data_r = reshape(data,(1,:))\n# cut the data into epochs\ndata_epochs,times = Unfold.epoch(data=data_r,tbl=evts,τ=(-0.4,0.8),sfreq=50);\n# missing or partially missing epochs are currenlty _only_ supported for non-mixed models!\nevts,data_epochs = Unfold.dropMissingEpochs(evts,data_epochs); nothing #hide","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"We can now run the LinearMixedModel on each time point","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"m,results = fit(UnfoldLinearMixedModel,f,evts,data_epochs,times) ","category":"page"},{"location":"lmm_tutorial/#Fixed-Effects","page":"LMM Tutorial","title":"Fixed Effects","text":"","category":"section"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"res_fixef = results[results.group.==\"fixed\",:]\nPlots.plot(res_fixef.colname_basis,res_fixef.estimate,\n        group=res_fixef.term,\n        layout=1,legend=:outerbottom)","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"We see the condition effects and some residual overlap activity in the fixed effects","category":"page"},{"location":"lmm_tutorial/#Random-Effects","page":"LMM Tutorial","title":"Random Effects","text":"","category":"section"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"And the random effect results","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"res_ranef = results[results.group.==\"subject\",:]\nPlots.plot(res_ranef.colname_basis,res_ranef.estimate,\n        group=res_ranef.term,\n        layout=1,legend=:outerbottom)","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"The random effects are very high in areas where we simulated overlap. (i.e. <-0.1 and >0.2)","category":"page"},{"location":"lmm_tutorial/#With-Overlap-Correction","page":"LMM Tutorial","title":"With Overlap Correction","text":"","category":"section"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"For overlap correction, we have to use a basis function once again.","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"basisfunction = firbasis(τ=(-0.05,.4),sfreq=40)\nf  = @formula 0~1+condA*condB+(1+condA*condB|subject);","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"warning: Warning\n","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"See the low sampling frequency? This is because the modelfit increases quadratically with the number of predictors","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"We can now run the mixed model.","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"Easy syntax: Specify formula, events, EEG-data & the basis function","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"@time mm,results = fit(UnfoldLinearMixedModel,(Any=>(f,basisfunction),evts,data); nothing #hide ","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"We receive an object containing the (very large) mixed model:","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"show(coeftable(mm.modelinfo))","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"But again, we also get a tidy-dataframe with the results","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"first(results,6)","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"and thus we can easily plot the fixed effect results.","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"\nres_fixef = results[results.group.==\"fixed\",:]\nPlots.plot(res_fixef.colname_basis,res_fixef.estimate,\n        group=res_fixef.term,\n        layout=1,legend=:outerbottom)","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"And the random effect results.","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"\nres_ranef = results[results.group.==\"subject\",:]\nPlots.plot(res_ranef.colname_basis,res_ranef.estimate,\n        group=res_ranef.term,\n        layout=1,legend=:outerbottom)","category":"page"},{"location":"lmm_tutorial/#What-is-happening-under-the-hood?","page":"LMM Tutorial","title":"What is happening under the hood?","text":"","category":"section"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"\nXdc = designmatrix(UnfoldLinearMixedModel,f,evts,basisfunction)","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"Formula-Terms are wrapped with a TimeExpandedTerm, which upon calling modelcols will timeexpand the designmatrix. There is one TimeExpandedTerm for the FixedEffects and one for each RandomEffectsTerm.","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"typeof(Xdc.formulas.rhs)","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"Visualizing the designmatrices. Fixed Effects:","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"\nPlots.heatmap(Matrix(Xdc.Xs[1][1:300,:]))","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"Random Effects","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"\nPlots.heatmap(Matrix(Xdc.Xs[2][1:2000,1:500]))","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"And finally, generate the linear mixed model manually & fit it.","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"mf = unfoldfit(Unfold.UnfoldLinearMixedModel,Xs,data)\nresults = condense_long(mf)\nfirst(results,6)","category":"page"},{"location":"lmm_tutorial/#Summary","page":"LMM Tutorial","title":"Summary","text":"","category":"section"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"There are four different model types currently \"fitable\"","category":"page"},{"location":"lmm_tutorial/","page":"LMM Tutorial","title":"LMM Tutorial","text":"Timeexpansion No, Mixed No  : fit(UnfoldLinearModel,f,evts,data_epoch,times)\nTimeexpansion Yes, Mixed No : fit(UnfoldLinearModel,f,evts,data,basisfunction)\nTimeexpansion No, Mixed Yes : fit(UnfoldLinearMixedModel,f,evts,data_epoch,times)\nTimeexpansion Yes, Mixed Yes: fit(UnfoldLinearMixedModel,f,evts,data,basisfunction)","category":"page"},{"location":"#Unfold-Documentation","page":"Unfold Documentation","title":"Unfold Documentation","text":"","category":"section"},{"location":"","page":"Unfold Documentation","title":"Unfold Documentation","text":"","category":"page"},{"location":"#Testing","page":"Unfold Documentation","title":"Testing","text":"","category":"section"},{"location":"lm_tutorial/#Overlap-Correction-with-Linear-Models","page":"LM Tutorial","title":"Overlap Correction with Linear Models","text":"","category":"section"},{"location":"lm_tutorial/#date:-2021-04-29","page":"LM Tutorial","title":"date: 2021-04-29","text":"","category":"section"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"!!! Note This is a tutorial, not so much a documentation.","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"using Plots; gr()\nPlots.reset_defaults();","category":"page"},{"location":"lm_tutorial/#Installation","page":"LM Tutorial","title":"Installation","text":"","category":"section"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"First we have to install some packages. in julia you would do this either by putting a ] in the REPL (\"julia-commandline\"). This should result in (Unfold) pkg> - but if you see (@v1.6) pkg>  instead, you still have to activate your environment (using cd(\"/path/to/your/project\") and ]activate . or ]activate /path/to/your/project/)","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"Note that you should have done this already to install Unfold in the first place. have a look at the Readme.md - there we use the Pkg.add(\"\") syntax, which is equivalent to the ] package manager. Now we are ready to add packages:","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"add StatsModels,MixedModels,DataFrames,DSP.conv,Plots","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"Next we have to make sure to be in the Unfold/docs folder, else the tutorial will not be able to find the data. Thus cd(\"./docs\") in case you cd'ed already to the Unfold project.","category":"page"},{"location":"lm_tutorial/#Setting-up-and-loading-the-data","page":"LM Tutorial","title":"Setting up & loading the data","text":"","category":"section"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"\nusing StatsModels, MixedModels, DataFrames\nimport DSP.conv\nimport Plots\nusing Unfold\ninclude(\"../../test/test_utilities.jl\"); # to load the simulated data","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"In this notebook we will fit regression models to (simulated) EEG data. We will see that we need some type of overlap correction, as the events are close in time to each other, so that the respective brain responses overlap. If you want more detailed introduction to this topic check out our paper","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"data, evts = loadtestdata(\"testCase2\",dataPath=\"../../test/data/\");","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"The data has little noise and the underlying signal is a pos-neg spike pattern","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"Plots.plot(range(1/50,length=200,step=1/50),data[1:200])\nPlots.vline!(evts[evts.latency.<=200,:latency]./50) # show events","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"Now have a look at the events","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"show(first(evts,6,),allcols=true)","category":"page"},{"location":"lm_tutorial/#Traditional-Mass-Univariate-Analysis","page":"LM Tutorial","title":"Traditional Mass Univariate Analysis","text":"","category":"section"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"In order to demonstrate why overlap correction is important, we will first epoch the data and fit a linear model to each time point. This is a \"traditional mass-univariate analysis\".","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"# we have multi channel support\ndata_r = reshape(data,(1,:))\n# cut the data into epochs\ndata_epochs,times = Unfold.epoch(data=data_r,tbl=evts,τ=(-0.4,0.8),sfreq=50);","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"!!! Note In julia, missing is supported throughout the ecosystem. Thus, we can have partial trials and they will be incorporated / ignored at the respective functions.","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"We define a formula that we want to apply to each point in time","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"f  = @formula 0~1+conditionA+conditionB # 0 as a dummy, we will combine wit data later","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"We fit the UnfoldLinearModel to the data","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"m,results = Unfold.fit(UnfoldLinearModel,f,evts,data_epochs,times); nothing #hide","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"The object has the following fields","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"m","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"Which contain the model, the original formula, the original events and returns extra a tidy-dataframe with the results","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"\n\nfirst(results,6)","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"We can also plot it:","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"Plots.plot(results.colname_basis,results.estimate,\n        group=results.term,\n        layout=1,legend=:outerbottom)","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"!!! Note (:colname_basis is used instead of :time [this might change]. The reason is that not all basisfunctions have a time dimension)","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"As can be seen a lot is going on here. As we will see later, most of the activity is due to overlap with the next event","category":"page"},{"location":"lm_tutorial/#Basis-Functions","page":"LM Tutorial","title":"Basis Functions","text":"","category":"section"},{"location":"lm_tutorial/#HRF-/-BOLD","page":"LM Tutorial","title":"HRF / BOLD","text":"","category":"section"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"We are now ready to define a basisfunction. There are currently only few basisfunction implemented. We first have a look at the BOLD-HRF basisfunction:","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"\n\nTR = 1.5\nbold = hrfbasis(TR) # using default SPM parameters\neventonset = 1.3\nPlots.plot(bold.kernel(eventonset))","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"Classically, we would convolve this HRF function with a impulse-vector, with impulse at the event onsets","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"\n\ny = zeros(100)\ny[[10,30,37,45]] .=1\ny_conv = conv(y,bold.kernel(0))\nPlots.plot(y_conv)","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"Which one would use as a regressor against the recorded BOLD timecourse.","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"Note that events could fall inbetween TR (the sampling rate). Some packages subsample the time signal, but in Unfold we can directly call the bold.kernel function at a given event-time, which allows for non-TR-multiples to be used.","category":"page"},{"location":"lm_tutorial/#FIR-Basis-Function","page":"LM Tutorial","title":"FIR Basis Function","text":"","category":"section"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"Okay, let's have a look at a different basis function: The FIR basisfunction.","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"\n\nbasisfunction = firbasis(τ=(-0.4,.8),sfreq=50,name=\"myFIRbasis\")\nPlots.plot(basisfunction.kernel(0))","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"Not very clear, better show it in 2D","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"\n\nbasisfunction.kernel(0)[1:10,1:10]","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"(all . are 0's)","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"The FIR basisset consists of multiple basisfunctions. That is, each event will now be timeexpanded to multiple predictors, each with a certain time-delay to the event onset. This allows to model any arbitrary linear overlap shape, and doesn't force us to make assumptions on the convolution kernel (like we had to do in the BOLD case)","category":"page"},{"location":"lm_tutorial/#Timeexpanded-/-Deconvolved-ModelFit","page":"LM Tutorial","title":"Timeexpanded / Deconvolved ModelFit","text":"","category":"section"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"Remember our formula from above","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"\n\nf","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"For the left-handside we use \"0\" as the data is separated from the events. This will in the future allow us to fit multiple channels easily.","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"And fit a UnfoldLinearModel. Not that instead of times as in the mass-univariate case, we have a BasisFunction object now.","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"\n\nm,results = fit(UnfoldLinearModel,Dict(Any=>(f,basisfunction),evts,data); nothing #hide","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"!!! Note in (Any=>(f,basisfunction), the Any means to use all rows in evts. In case you have multiple events, you'd want to specify multiple basisfunctions e.g. ","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"Dict(\"stimulus\"=>(f1,basisfunction1),\n \"response\"=>(f2,basisfunction2))","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"You likely have to specify a further argument to fit: eventcolumn=\"type\" with type being the column in evts that codes for the event (stimulus / response in this case)","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"\n\nPlots.plot(results.colname_basis,results.estimate,\n        group=results.term,\n        layout=1,legend=:outerbottom)","category":"page"},{"location":"lm_tutorial/","page":"LM Tutorial","title":"LM Tutorial","text":"Cool! All overlapping activity has been removed and we recovered the simulated underlying signal.","category":"page"}]
}
